# -*- coding: utf-8 -*-
"""gmaps bert

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dykmFystqCgJj6bazlnzN7mXcu7L7W3t
"""

!pip install transformers nltk torch openpyxl

import pandas as pd
import torch
import re
import nltk
from bs4 import BeautifulSoup
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')

from google.colab import drive
drive.mount('/content/drive')

import glob

file_paths = glob.glob("/content/drive/MyDrive/Sentiment Analysis/gmaps bert (1)/*.csv")

# Check if any files were found
if not file_paths:
    print("No .xlsx files found in the specified directory. Please check the path.")
    # You might want to exit or handle this case appropriately
    # For example, you could create an empty DataFrame or raise an error
    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors
else:
    dfs = [pd.read_csv(file) for file in file_paths]
    df = pd.concat(dfs, ignore_index=True)

# prompt: head

print(df)

df = df.dropna(subset=['review_text'])

df.to_csv('cleaned_reviews.csv', index=False)

from bs4 import BeautifulSoup

def clean_text(text):
    text = str(text)  # Ensure text is a string
    text = BeautifulSoup(text, "html.parser").get_text()  # Remove HTML tags
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    text = text.lower().strip()  # Lowercase and strip spaces

    tokens = nltk.word_tokenize(text, language='english')
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df = df.dropna(subset=['review_text'])

duplicated_rows = df[df.duplicated(subset=['review_text'], keep='first')]
df = df.drop_duplicates(subset=['review_text'], keep='first')

df['cleaned_review'] = df['review_text'].apply(clean_text)

print(duplicated_rows)

print(df)

# prompt: save df as excel

df.to_excel('cleaned_reviews.xlsx', index=False)

# Create sentiment labels based on ratings
df['sentiment'] = df['rating'].apply(lambda x: 0 if x < 3 else (1 if x > 3 else 2))

# Split the data into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Load the BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize all text at once for efficiency
encoded_data = tokenizer.batch_encode_plus(
    df['cleaned_review'].tolist(),  # Use cleaned text
    add_special_tokens=True,
    max_length=256,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)

# Convert to PyTorch tensors
input_ids = encoded_data['input_ids']
attention_masks = encoded_data['attention_mask']

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Tokenize the training and testing data
train_encodings = tokenizer(list(train_df['cleaned_review']), truncation=True, padding=True)
test_encodings = tokenizer(list(test_df['cleaned_review']), truncation=True, padding=True)

# Create PyTorch datasets
train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),
                             torch.tensor(train_encodings['attention_mask']),
                             torch.tensor(train_df['sentiment'].values))
test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),
                            torch.tensor(test_encodings['attention_mask']),
                            torch.tensor(test_df['sentiment'].values))

# Create PyTorch dataloaders
train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16, pin_memory=True)
val_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16, pin_memory=True)

# Set up the optimizer and learning rate scheduler
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

import torch
from sklearn.metrics import accuracy_score, classification_report, f1_score
from torch.cuda.amp import autocast, GradScaler

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Initialize gradient scaler for mixed precision training
scaler = GradScaler()

epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        batch = tuple(t.to(device) for t in batch)  # Move batch to device
        input_ids, attention_mask, labels = batch

        optimizer.zero_grad()

        with autocast():  # Mixed precision training
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss

        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch + 1}: Average Training Loss = {avg_loss:.4f}")

    # Evaluation phase
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in val_dataloader:
            batch = tuple(t.to(device) for t in batch)
            input_ids, attention_mask, labels = batch

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            predicted_labels = torch.argmax(logits, dim=1)

            predictions.extend(predicted_labels.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')  # F1-score

    print(f'Epoch {epoch + 1}: Accuracy = {accuracy:.4f}, F1-score = {f1:.4f}')
    print(classification_report(true_labels, predictions))

    scheduler.step()  # Step scheduler after epoch

# Save the fine-tuned model
torch.save(model.state_dict(), 'fine_tuned_bert_model.pth')

# Load the fine-tuned model
model.load_state_dict(torch.load('fine_tuned_bert_model.pth'))

# Run sentiment analysis on the entire review text
all_encodings = tokenizer(list(df['cleaned_review']), truncation=True, padding=True)
all_dataset = TensorDataset(torch.tensor(all_encodings['input_ids']),
                           torch.tensor(all_encodings['attention_mask']))
all_dataloader = DataLoader(all_dataset, sampler=SequentialSampler(all_dataset), batch_size=16)

model.eval()
all_predictions = []
with torch.no_grad():
    for batch in all_dataloader:
        input_ids, attention_mask = batch
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_labels = torch.argmax(logits, dim=1)
        all_predictions.extend(predicted_labels.cpu().numpy())

df['predicted_sentiment'] = all_predictions

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix

# Compute accuracy
accuracy = accuracy_score(true_labels, predictions)

# Compute precision, recall, and F1-score
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')

# Generate classification report
class_report = classification_report(true_labels, predictions)

# Compute confusion matrix
conf_matrix = confusion_matrix(true_labels, predictions)

# Print evaluation results
print(f"ðŸ“Š Model Evaluation Results:")
print(f"ðŸ”¹ Accuracy: {accuracy:.4f}")
print(f"ðŸ”¹ Precision: {precision:.4f}")
print(f"ðŸ”¹ Recall: {recall:.4f}")
print(f"ðŸ”¹ F1 Score: {f1:.4f}")
print("\nðŸ”¹ Classification Report:\n", class_report)
print("\nðŸ”¹ Confusion Matrix:\n", conf_matrix)

!pip install contractions datasets emoji gensim matplotlib nltk numpy pandas pyLDAvis seaborn transformers wordcloud

# ==================================
#           2. Imports
# ==================================

# Standard Python libraries
import re
import numpy as np
import pandas as pd
from collections import Counter
from google.colab import drive

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# NLTK imports
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.corpus import wordnet

# Topic Modeling (Gensim and pyLDAvis)
from gensim import corpora, models
from gensim.models import Phrases

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Transformers and HuggingFace Datasets
from datasets import Dataset

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

# Other libraries
import contractions
import emoji

# ==================================
#     3. Download NLTK Resources
# ==================================

nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt')
nltk.download('punkt_tab')

# ==================================
#       4. Verify Environment
# ==================================

print("Installed and imported libraries successfully.")
print("Numpy version:", np.__version__)
print("Pandas version:", pd.__version__)

# prompt: remove empty rows in review text

# Remove rows where 'review_text' is empty or contains only whitespace
df = df[df['cleaned_review'].str.strip() != ""]

# List of most used words
vectorizer = CountVectorizer(stop_words='english')
word_counts = vectorizer.fit_transform(df['cleaned_review'].dropna())
word_sum = np.asarray(word_counts.sum(axis=0)).flatten()
word_list = vectorizer.get_feature_names_out()
word_freq = pd.DataFrame({'word': word_list, 'count': word_sum}).sort_values(by='count', ascending=False)
print("20 Most Used Words in Reviews:\n", word_freq.head(20))

# Extract the top 20 most used words
top_20_words = word_freq.head(20)['word'].tolist()

# prompt: show me the cathegories in sentiment

print(df['sentiment'].unique())

# Define stopwords and add custom words
custom_stopwords = set(stopwords.words('english'))
custom_stopwords.update(['villa', 'lake', 'go', 'nan', 'gardens', 'place', 'view', 'como', 'day', 'just','wa', 'around', 'one', 'took', 'way'])

# Tokenization and cleaning function
def tokenize_and_clean(text):
    if pd.notnull(text):
        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
        tokens = [word for word in tokens if word.isalnum() and word not in custom_stopwords]  # Filter out stopwords and non-alphanumeric tokens
        return tokens
    return []

# Apply tokenization and cleaning to the dataset
df['Tokens'] = df['cleaned_review'].apply(tokenize_and_clean)

# Prepare data for topic modeling
dictionary = corpora.Dictionary(df['Tokens'])
corpus = [dictionary.doc2bow(tokens) for tokens in df['Tokens']]

# Apply LDA topic modeling
num_topics = 5  # Set the number of topics
lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)

# Print the topics
print("\nGenerated Topics:")
for i, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):
    print(f"Topic {i + 1}: {topic}")

# Visualize topics
pyLDAvis.enable_notebook()
lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(lda_vis)

# 1. Define stopwords and add custom words
custom_stopwords = set(stopwords.words('english'))
custom_stopwords.update([
    'villa', 'lake', 'go', 'gardens', 'place', 'view',
    'como', 'day', 'just','wa', 'around', 'one', 'took', 'way', 'nan'
])

# 2. Tokenization and cleaning function
def tokenize_and_clean(text):
    if pd.notnull(text):
        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
        tokens = [word for word in tokens
                  if word.isalnum() and word not in custom_stopwords]
        return tokens
    return []

# Assume `data` is your DataFrame with columns ['Cleaned_Review_Text', 'Sentiment'].
# Make sure 'Sentiment' contains categories like "Positive", "Negative", etc.

# 3. Tokenize the entire DataFrame
df['Tokens'] = df['cleaned_review'].apply(tokenize_and_clean)

# 4. Get the unique sentiment categories
sentiment_categories = df['sentiment'].unique()

# 5. Loop through each sentiment category
for sentiment in sentiment_categories:
    print(f"\n=== SENTIMENT CATEGORY: {sentiment} ===")

    # Subset the data for the current sentiment
    subset = df[df['sentiment'] == sentiment]

    # Skip if no data for this sentiment (edge case)
    if subset.empty:
        print(f"No reviews found for sentiment: {sentiment}")
        continue

    # Prepare the dictionary and corpus for LDA
    dictionary = corpora.Dictionary(subset['Tokens'])
    corpus = [dictionary.doc2bow(tokens) for tokens in subset['Tokens']]

    # 6. Train the LDA model
    num_topics = 5  # set the number of topics as needed
    lda_model = models.LdaModel(
        corpus,
        num_topics=num_topics,
        id2word=dictionary,
        passes=10,
        random_state=42  # for reproducibility
    )

    # 7. Print the topics
    print(f"\nGenerated Topics for sentiment = {sentiment}:")
    for i, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):
        print(f"Topic {i + 1}: {topic}")

    # 8. Visualize the topics (optional)
    #    If you're working in a Jupyter notebook, you can display the visualization inline.
    try:
        pyLDAvis.enable_notebook()
        lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)
        display(pyLDAvis.display(lda_vis))
    except Exception as e:
        print("Could not display pyLDAvis. Make sure you're running in a notebook environment.")
        print(f"Error details: {e}")

data = df

# Display the first few rows
print(data.head())

print(len(data))

# Count the number of reviews for each rating
reviews_ratings = data['rating'].value_counts().reset_index()
reviews_ratings.columns = ['rating', 'count']  # Rename columns for clarity
reviews_ratings.sort_values('rating', inplace=True)  # Ensure ratings are in order

# Prepare for plotting
fig, ax = plt.subplots(figsize=(10, 6))

# Create a bar plot for the number of reviews per rating
sns.barplot(
    data=reviews_ratings,
    x='rating',
    y='count',
    palette='plasma',
    ax=ax
)

# Titles and labels
ax.set_title('Number of Reviews per Rating', fontsize=14)
ax.set_xlabel('Rating', fontsize=12)
ax.set_ylabel('Number of Reviews', fontsize=12)

# Add data labels to each bar
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, padding=3)

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()